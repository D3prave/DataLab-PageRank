# Academic Citation Crawler, Dashboard & PageRank

A scalable system for collecting academic citation data via the Semantic Scholar API, monitoring crawling performance in real time, and computing PageRank scores on massive citation graphs using high-performance tools like Redis, PostgreSQL, FastAPI, and graph-tool.

---

## ğŸ“š Table of Contents

- [ğŸ“Œ Overview](#-overview)
- [âœ¨ Features](#-features)
- [ğŸ•· Crawler Script (`crawler.py`)](#-crawlerpy)
- [ğŸ“Š Dashboard Script (`dashboard.py`)](#-dashboardpy)
- [ğŸ›‘ Remote Service Controller Script (`start_stop_crawler.py`)](#-start_stop_crawlerpy)
- [ğŸ§¾ Author Paper Fetcher (`fetch_author_papers.py`)](#-fetch_author_paperspy)
- [ğŸ§® PageRank Computation (`HPC_graph_tool.py`)](#-hpc_graph_toolpy)
- [ğŸ”Œ API & Interfaces](#-api--interfaces)
- [ğŸ¤– Technologies Used](#-technologies-used)
- [ğŸ“ Requirements](#-requirements)
- [ğŸ“ Architecture](#-architecture)
- [ğŸ“ Project Structure](#-project-structure)

---

## ğŸ“Œ Overview

This project implements a full pipeline for crawling and analyzing academic citation data at scale.

At its core is a resilient crawler that retrieves paper metadata and citation relationships from the [Semantic Scholar API](https://api.semanticscholar.org/). It supports queue-based task distribution, deduplication through RedisBloom and PostgreSQL, and robust retry handling with `tenacity`.

To monitor system health and crawling performance, a FastAPI-based dashboard provides real-time stats, including memory usage across remote servers accessed via SSH.

After the crawl is complete, the system launches a high-performance analysis pipeline on the extracted citation graph and its PageRank scores using HPC resources. We leverage graph-tool for scalable graph algorithms, networkx and scipy for fast network metrics, and pandas for large-scale data manipulation. For visualization and interactive exploration, we integrate pyvis, matplotlib (with optional seaborn styling), and plotly. The PageRank stage runs on a supercomputer, producing normalized influence scores that let users instantly identify the most impactful papers by citation structure.

Together, these components form a modular and extensible platform for academic network analysis â€” from crawling raw citation data to ranking papers by influence.

---

## âœ¨ Features

- âœ… Distributed crawling with queue-based task management
- ğŸš¦ Queue management with Redis and JSON-encoded payloads
- ğŸ§  Deduplication with Redis Bloom filters + SQL fallback
- ğŸ” Retry logic with exponential backoff using tenacity
- ğŸ•¸ï¸ Citation graph generation (directed edges) with pagination for large papers
- ğŸ“Š Real-time dashboard with memory, rate, and paper stats
- ğŸ“¦ Background memory checks on remote servers via SSH

---

## ğŸ•· crawler.py

The main script that handles fetching, deduplication, citation parsing, and task queue management.

### ğŸ§© Key Components

- âš¡ send_request: Rate-limited API requests with retry logic
- ğŸ“œ fetch_references_paginated: Handles large reference sets (>1000)
- ğŸ§  filter_new_ids: Bloom filter + SQL fallback deduplication
- ğŸ”’ safe_insert_citations: Robust insert with deadlock handling
- ğŸ·ï¸ mark_processed: Marks paper as crawled in both Redis and SQL
- ğŸš€ main(): Main crawl loop with seed support, resume, and batching
 
### â–¶ How to Use

Start a **fresh** crawl with seed IDs:
```bash
python crawler.py --fresh <seed_id1> <seed_id2>
```
Resume a previously interrupted crawl:
```bash
python crawler.py --resume
```

---

## ğŸ“Š dashboard.py

A FastAPI app providing real-time monitoring for crawler performance and system resource usage.

### ğŸ“¡ Endpoints

- ğŸŒ GET - HTML Dashboard UI
- ğŸ“ˆ GET /status - Returns crawler metrics as JSON
  
### ğŸ“ˆ Metrics Shown

- âœ… Number of processed papers
- ğŸ”— Citation edges discovered
- ğŸ§  Memory pressure (macOS local)
- ğŸ§  RAM usage on remote servers via SSH
- âš¡ Papers/second & ğŸ“ˆ papers/hour
- ğŸ•’ Estimated time per 1000 papers

### ğŸ§µ Background Tasks

- â²ï¸ remote_ram_background_updater() â€“ polls RAM usage every 60s
- â²ï¸ speed_background_updater() â€“ updates crawl rate every 15s

### â–¶ How to Use

Run the **Dashboard** server:
```bash
uvicorn dashboard:app --host 0.0.0.0 --port xxxx --reload
```

---
## ğŸ›‘ start_stop_crawler.py

This utility script manages the crawler.service systemd unit on multiple remote servers via SSH. It allows you to start or stop the crawler daemon across all instances with a single command.

### ğŸ”§ Configuration

The script uses a dictionary (HOST_KEY_MAP) to map server IPs to their corresponding private SSH key files:
```
HOST_KEY_MAP = {
    "IP": "KEY",
    "IP": "KEY",
    "IP": "KEY",
}
```
Make sure all key files are present and accessible.
  
### â–¶ How to Use

Start the crawler.service on all configured remote hosts:
```
python start_stop_crawler.py --on
```
Stop the crawler.service.
```
python start_stop_crawler.py --off
```

### âš ï¸ Note

Remote servers must have:
- ğŸ” SSH access enabled
- ğŸ› ï¸ Python systemd unit defined as crawler.service

Local machine must have:
- ğŸ”‘ Private SSH key access for each host
- ğŸ“¦ paramiko installed

---

## ğŸ§¾ fetch_author_papers.py

This utility script fetches papers written by specific authors from the Semantic Scholar API. It saves:
- ğŸ“ Detailed metadata in a CSV file (papers_by_authors.csv)
- ğŸ†” Paper IDs only in a plain text file (paper_ids.txt) for seeding the crawler

### â–¶ How to Use

Edit the author_ids list in the script to include the authors you are interested in:

```
author_ids = [
    "ID1", "ID2", "ID3", ...
]
```

---

## ğŸ§® HPC_graph_tool.py

This script computes PageRank scores on the full citation graph using the high-performance graph-tool library, suitable for large-scale academic datasets and HPC environments. It:
- ğŸ—ï¸ Builds a directed graph from citation edges
- ğŸ“ˆ Computes PageRank using graph_tool.pagerank()
- ğŸ¯ Normalizes scores to sum = 1
- ğŸ”— Joins PageRank values with metadata

### Inputs

- `--input`: CSV file containing citation edges with columns: (`citing_id` , `cited_id`)
- `--metadata`: CSV file with metadata for papers, containing: (`paper_id`, `fields_of_study`)
- `--output`: Output directory to store logs, results, and graph file

### Outputs

- `pagerank_merged.csv`: PageRank scores merged with paper metadata
- `graph.graphml`: GraphML file for visualization
- `pagerank.log`: Execution logs

### âš ï¸ Note

- âš¡ This computation was run on a supercomputer using SLURM job scheduling due to the large size of the citation graph (millions of nodes and edges).
- ğŸ³ The environment was containerized with Docker, ensuring consistent dependency management and compatibility for graph-tool.

## ğŸ”Œ API & Interfaces

### ğŸŒ Semantic Scholar API
Base URL: https://api.semanticscholar.org/graph/v1
- GET /paper/{id}/references: For paginated citation lists
- POST /paper/batch: For batched paper metadata

### ğŸ” Redis
- paper_queue	- FIFO queue of papers to crawl
- processed_bloom	- RedisBloom filter to avoid duplicate paper IDs
- queued_bloom: RedisBloom filter to avoid duplicate enqueueing

### ğŸ—ƒï¸ PostgreSQL
- processed_papers - Stores paper_id and its fields_of_study
- citations - Stores directed edges (citing_id â†’ cited_id)

### ğŸŒ FastAPI Dashboard
- GET - HTML dashboard UI
- GET /status -	Returns system stats in JSON (see below)

---

## ğŸ¤– Technologies Used

- **Python 3**
- **PostgreSQL** (`psycopg2`, `asyncpg`)
- **Redis** + Bloom Filter
- **FastAPI** (Dashboard backend)
- **requests** (API access)
- **asyncssh** (RAM monitoring via SSH)
- **Semantic Scholar API**
- **Tenacity** (for robust retry logic)
- **Uvicorn** (for serving dashboard)
- **paramiko** (for SSH protocols)
- **subprocess/sysctl** (macOS memory pressure)

---

## ğŸ“ Requirements

- crawler.py: requests, redis, psycopg2, tenacity
- dashboard.py: fastapi, uvicorn, asyncssh
- start_stop_crawler.py: paramiko
- fetch_author_papers.py: requests

```
pip install -r requirements.txt
```
---

## ğŸ“ Architecture
```
               +-----------------------+
               |  Semantic Scholar API |
               +----------+------------+
                          |
                          v
               +-----------------------+
               |      Crawler.py       |
               |-----------------------|
               |  - Batched Fetching   |
               |  - Reference Parsing  |
               |  - Deduplication      |
               |  - Queue Handling     |
               +----------+------------+
                          |
           +--------------+--------------+
           |                             |
           v                             v
  +-------------------+       +------------------------+
  |     PostgreSQL    |       |         Redis          |
  |-------------------|       |------------------------|
  |  - Processed IDs  |       |  - Task Queue (FIFO)   |
  |  - Citations      |       |  - Bloom Filter        |
  +-------------------+       +------------------------+
           |
           v
 +--------------------------+
 |      FastAPI Dashboard   |
 |--------------------------|
 | - Crawl Stats            |
 | - Crawl Speed            |
 | - Remote RAM Monitoring  |
 +--------------------------+
           |
           v
 +--------------------------+
 |     Web Browser UI       |
 +--------------------------+
```
---

## ğŸ“ Project Structure
```
.
â”œâ”€â”€ crawler.py
â”œâ”€â”€ dashboard.py
â”œâ”€â”€ fetch_author_papers.py
â”œâ”€â”€ start_stop_crawler.py 
â”œâ”€â”€ HPC_graph_tool.py
â””â”€â”€ requirements.txt
```

---

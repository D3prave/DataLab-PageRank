# Academic Citation Crawler & Dashboard

This project implements a scalable academic citation crawler powered by the Semantic Scholar API. It supports efficient deduplication using RedisBloom, structured storage in PostgreSQL, and real-time monitoring with a FastAPI dashboard, including memory stats from remote instances via SSH.

---

## ğŸ“š Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [How to Run](#-how-to-run)
- [Crawler Script (`crawler.py`)](#-crawlerpy)
- [Dashboard Script (`dashboard.py`)](#-dashboardpy)
- [Remove Service Controller Script (`start_stop_crawler.py`)](#-start_stop_crawlerpy)
- [Author Paper Fetcher (`fetch_author_papers.py`)](#-fetch_author_paperspy)
- [API & Interfaces](#-api--interfaces)
- [Technologies Used](#-technologies-used)
- [Architecture](#-architecture)
- [Project Structure](#-project-structure)

---

## ğŸ“Œ Overview

This project implements a robust academic crawler to collect citation data via the [Semantic Scholar API](https://api.semanticscholar.org/). The data is processed, stored in PostgreSQL, and de-duplicated using Redis with a Bloom filter. A live dashboard (FastAPI) monitors crawler performance and remote server resources.

---

## âœ¨ Features

- âœ… Distributed crawling with queue-based task management
- ğŸš¦ Queue management with Redis and JSON-encoded payloads
- ğŸ§  Deduplication with Redis Bloom filters + SQL fallback
- ğŸ” Retry logic with exponential backoff using tenacity
- ğŸ•¸ï¸ Citation graph generation (directed edges) with pagination for large papers
- ğŸ“Š Real-time dashboard with memory, rate, and paper stats
- ğŸ“¦ Background memory checks on remote servers via SSH

---

## â–¶ How to Run

### ğŸ”§ Requirements

- PostgreSQL and Redis running locally
- Semantic Scholar API Key (`API_KEY`) (API key can be left blank for public usage)
- Python dependencies (pip install -r requirements.txt)
- Python packages: see `requirements.txt`

### ğŸš€ Commands

Start a **fresh** crawl with seed IDs:
```bash
python crawler.py --fresh <seed_id1> <seed_id2>
```
Resume a previously interrupted crawl:
```bash
python crawler.py --resume
```
Run the **Dashboard** server:
```bash
uvicorn dashboard:app --host 0.0.0.0 --port xxxx --reload
```

---

## ğŸ§ª crawler.py

The main script that handles fetching, deduplication, citation parsing, and task queue management.

### ğŸ§© Key Components

- send_request: Rate-limited API requests with retry logic
- fetch_references_paginated: Handles large reference sets (>1000)
- filter_new_ids: Bloom filter + SQL fallback deduplication
- safe_insert_citations: Robust insert with deadlock handling
- mark_processed: Marks paper as crawled in both Redis and SQL
- main(): Main crawl loop with seed support, resume, and batching

---

## ğŸ“Š dashboard.py

A FastAPI app providing real-time monitoring for crawler performance and system resource usage.

### ğŸ“¡ Endpoints

- GET - HTML Dashboard UI
- GET /status - Returns crawler metrics as JSON
  
### ğŸ“ˆ Metrics Shown

- âœ… Number of processed papers
- ğŸ”— Citation edges discovered
- ğŸ§  Memory pressure (macOS local)
- ğŸ§  RAM usage on remote servers via SSH
- âš¡ Papers/second & ğŸ“ˆ papers/hour
- ğŸ•’ Estimated time per 1000 papers

### ğŸ§µ Background Tasks

- remote_ram_background_updater() â€“ polls RAM usage every 60s
- speed_background_updater() â€“ updates crawl rate every 15s

---
## ğŸ›‘ start_stop_crawler.py

This utility script manages the crawler.service systemd unit on multiple remote servers via SSH. It allows you to start or stop the crawler daemon across all instances with a single command.

### ğŸ“¡ Configuration

The script uses a dictionary (HOST_KEY_MAP) to map server IPs to their corresponding private SSH key files:
```
HOST_KEY_MAP = {
    "IP": "KEY",
    "IP": "KEY",
    "IP": "KEY",
}
```
Make sure all key files are present and accessible.
  
### How to Use

Start the crawler.service on all configured remote hosts:
```
python start_stop_crawler.py --on
```
Stop the crawler.service
```
python start_stop_crawler.py --off
```

### Note

Remote servers must have:
- SSH access enabled
- Python systemd unit defined as crawler.service

Local machine must have:
- Private SSH key access for each host
- paramiko installed

---

## ğŸ§¾ fetch_author_papers.py

This utility script fetches papers written by specific authors from the Semantic Scholar API. It saves:
- Detailed metadata in a CSV file (papers_by_authors.csv)
- Paper IDs only in a plain text file (paper_ids.txt) for seeding the crawler

### How to Use

Edit the author_ids list in the script to include the authors you are interested in:
```
author_ids = [
    "ID1", "ID2", "ID3", ...
]
```

---

## ğŸ”Œ API & Interfaces

### ğŸ“¡ Semantic Scholar API
Base URL: https://api.semanticscholar.org/graph/v1
- GET /paper/{id}/references: For paginated citation lists
- POST /paper/batch: For batched paper metadata

### ğŸ” Redis
- paper_queue	- FIFO queue of papers to crawl
- processed_bloom	- RedisBloom filter to avoid duplicate paper IDs
- queued_bloom: RedisBloom filter to avoid duplicate enqueueing

### ğŸ—ƒï¸ PostgreSQL
- processed_papers - Stores paper_id and its fields_of_study
- citations - Stores directed edges (citing_id â†’ cited_id)

### ğŸŒ FastAPI Dashboard
- GET - HTML dashboard UI
- GET /status -	Returns system stats in JSON (see below)

---

## ğŸ¤– Technologies Used

- **Python 3**
- **PostgreSQL** (`psycopg2`, `asyncpg`)
- **Redis** + Bloom Filter
- **FastAPI** (Dashboard backend)
- **requests** (API access)
- **asyncssh** (RAM monitoring via SSH)
- **Semantic Scholar API**
- **Tenacity** (for robust retry logic)
- **Uvicorn** (for serving dashboard)
- **paramiko** (for SSH protocols)
- **subprocess/sysctl** (macOS memory pressure)

---

## ğŸ“ Architecture
```
               +-----------------------+
               |  Semantic Scholar API |
               +----------+------------+
                          |
                          v
               +-----------------------+
               |      Crawler.py       |
               |-----------------------|
               |  - Batched Fetching   |
               |  - Reference Parsing  |
               |  - Deduplication      |
               |  - Queue Handling     |
               +----------+------------+
                          |
           +--------------+--------------+
           |                             |
           v                             v
  +-------------------+       +------------------------+
  |     PostgreSQL    |       |         Redis          |
  |-------------------|       |------------------------|
  |  - Processed IDs  |       |  - Task Queue (FIFO)   |
  |  - Citations      |       |  - Bloom Filter        |
  +-------------------+       +------------------------+
           |
           v
 +--------------------------+
 |      FastAPI Dashboard   |
 |--------------------------|
 | - Crawl Stats            |
 | - Crawl Speed            |
 | - Remote RAM Monitoring  |
 +--------------------------+
           |
           v
 +--------------------------+
 |     Web Browser UI       |
 +--------------------------+
```
---

## ğŸ“ Project Structure
```
.
â”œâ”€â”€ crawler.py
â”œâ”€â”€ dashboard.py
â”œâ”€â”€ fetch_author_papers.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ start_stop_crawler.py
```

---
